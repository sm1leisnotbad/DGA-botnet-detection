{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXNlPk59yxKC",
        "outputId": "ae9b21f3-dd6d-4736-d76b-d3e79a1e2c2d"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/andrewaeva/DGA\n",
        "!pip install wordsegment\n",
        "!pip install pyenchant\n",
        "!apt install hunspell\n",
        "!apt install hunspell-es\n",
        "!apt install hunspell-de-de\n",
        "!apt install hunspell-fr\n",
        "!apt install hunspell-it\n",
        "!apt install hunspell-uk\n",
        "!apt-get install enchant-2\n",
        "!pip install syllables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AirhS7vXZ0fE",
        "outputId": "5007a753-b241-4f8a-e020-68b2320dc0f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kSIi9rLJSR5",
        "outputId": "7745801d-19b5-46ba-d074-303f846495a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "data = \"\"\"cryptolocker    1\n",
        "zeus            2\n",
        "pushdo          3\n",
        "rovnix          4\n",
        "tinba           5\n",
        "matsnu          7\n",
        "ramdo           8\"\"\"\n",
        "\n",
        "# Split the data into lines\n",
        "lines = data.split('\\n')\n",
        "\n",
        "# Extract names and labels into separate lists\n",
        "names = []\n",
        "labels = []\n",
        "\n",
        "for line in lines:\n",
        "    parts = line.split()\n",
        "    if len(parts) == 2:\n",
        "        name, label = parts\n",
        "        names.append(name)\n",
        "        labels.append(int(label))\n",
        "\n",
        "lines = []\n",
        "print(len(names))\n",
        "for i in range(len(names)):\n",
        "    with open(\"DGA/dga_wordlists/\"+names[i]+\".txt\", \"r\") as f:\n",
        "        lines += [next(f).strip()+' '+str(labels[i]) for _ in range(30000)]\n",
        "\n",
        "\n",
        "with open(\"DGA/all_legit.txt\") as f:\n",
        "    lines += [next(f).strip() for _ in range(300123)]\n",
        "\n",
        "with open(\"/content/drive/MyDrive/dataset/datset1.txt\", \"w\") as f:\n",
        "    f.write('\\n'.join(lines))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX66ykc3mvxg",
        "outputId": "51de4e60-2274-4251-f3d2-27ca6ce76c24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# features\n",
        "\n",
        "\n",
        "\n",
        "# domain length\n",
        "def getDomainLength(domain):\n",
        "    return len(domain)\n",
        "\n",
        "\n",
        "from wordsegment import load, segment\n",
        "import wordsegment\n",
        "\n",
        "def domain2list(domain):\n",
        "    if (len(domain)>0):\n",
        "        s = wordsegment.segment(domain)\n",
        "        return s\n",
        "    return []\n",
        "# number of length\n",
        "def getNumOfHyphen(domain):\n",
        "    return domain.count(\"-\")\n",
        "# number of numeric tokens\n",
        "import re\n",
        "def getNumOfDigit(domain):\n",
        "    n = re.findall(r\"\\d+\",domain)\n",
        "    return len(n)\n",
        "\n",
        "# rare of word\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('brown')\n",
        "def getRareOfWords(domain_word_list):\n",
        "    randomWordCount = 0\n",
        "    rareWordCount = 0\n",
        "    commonWordCount = 0\n",
        "    for i in domain_word_list:\n",
        "        if domain_freq.get(i) is None:\n",
        "            randomWordCount += 1\n",
        "        elif domain_freq.get(i) < 10000:\n",
        "            rareWordCount +=1\n",
        "        else:\n",
        "            commonWordCount +=1\n",
        "    global rareness\n",
        "    rareness = [randomWordCount, rareWordCount, commonWordCount]\n",
        "    return rareness\n",
        "\n",
        "def loadRareness(domain_word_list):\n",
        "    getRareOfWords(domain_word_list)\n",
        "\n",
        "# common ratio\n",
        "def getCommonRatio(domain_word_list):\n",
        "    return rareness[2] / len(domain_word_list)\n",
        "\n",
        "# rare ratio\n",
        "def getRareRatio(domain_word_list):\n",
        "    return rareness[1] / len(domain_word_list)\n",
        "# domain type\n",
        "def isDomainTypeIllicit(domaintype):\n",
        "    if domaintype in [\"com\",\"org\",\"ru\",\"net\",\"biz\"]:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "import enchant\n",
        "\n",
        "\n",
        "def getMeaningRatio(domain_word_list: list) -> list:\n",
        "    mean = 0\n",
        "    mispelled = 0\n",
        "    for i in domain_word_list:\n",
        "        if check_dict.check(i) or check_dict.check(i.capitalize()):\n",
        "            mean+=1\n",
        "        else: mispelled += 1\n",
        "    return [mean/len(domain_word_list), mispelled/len(domain_word_list)]\n",
        "\n",
        "# no random\n",
        "def getNumberRandomWord():\n",
        "    return rareness[2]\n",
        "\n",
        "\n",
        "# calculate weight value of each letter\n",
        "\n",
        "qwerty_keyboard = {\n",
        "    '1': 3, '2': 3, '3': 3, '4': 3, '5': 3, '6': 3, '7': 3, '8': 3, '9': 3, '0': 3, '-':3, '_': 4,\n",
        "    'q': 2, 'w': 2, 'e': 2, 'r': 2, 't': 2, 'y': 2, 'u': 2, 'i': 2, 'o': 2, 'p': 2,\n",
        "    'a': 1, 's': 1, 'd': 1, 'f': 1, 'g': 1, 'h': 1, 'j': 1, 'k': 1, 'l': 1,\n",
        "    'z': 2, 'x': 2, 'c': 2, 'v': 2, 'b': 2, 'n': 2, 'm': 2\n",
        "}\n",
        "def calculateWeightLetters(domain):\n",
        "    weight = 0\n",
        "    for i in domain:\n",
        "        a = qwerty_keyboard.get(i)\n",
        "        if type(a) is not int:\n",
        "          print(domain)\n",
        "          break\n",
        "        weight += a\n",
        "    return weight\n",
        "\n",
        "def calculateConsecutive(domain: str):\n",
        "    points = 0\n",
        "    consecutive_count = 0\n",
        "    for i in range(len(domain)):\n",
        "        if i > 0 and qwerty_keyboard.get(domain[i]) == qwerty_keyboard.get(domain[i-1]):\n",
        "            consecutive_count += 1\n",
        "        else:\n",
        "            consecutive_count = 1\n",
        "        if consecutive_count == 3:\n",
        "            points += 1\n",
        "\n",
        "    return points\n",
        "\n",
        "\n",
        "# digraph frequency\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_digraph_frequency(text):\n",
        "    digraphs = [text[i:i+2] for i in range(len(text)-1)]\n",
        "    return Counter(digraphs)\n",
        "def calculate_trigraph_frequency(text):\n",
        "    trigraphs = [text[i:i+3] for i in range(len(text)-2)]\n",
        "    return Counter(trigraphs)\n",
        "def calculate_char_frequency(text):\n",
        "    chars = [text[i] for i in range(len(text))]\n",
        "    return Counter(chars)\n",
        "\n",
        "# difficult of word\n",
        "def getWeightDifficult(domain, domain_word_list):\n",
        "    domain_len = getDomainLength(domain)\n",
        "    w = getNumOfDigit(domain) / domain_len\n",
        "    h = getNumOfHyphen(domain) / domain_len\n",
        "\n",
        "    dgraph = calculate_digraph_frequency(domain)\n",
        "    trigraph = calculate_digraph_frequency(domain)\n",
        "    s_dgraph = sum([value for key, value in dgraph.items()])\n",
        "    s_trigraph = sum([value for key, value in trigraph.items()])\n",
        "\n",
        "    d = ( s_dgraph + s_trigraph) / domain_len\n",
        "    list_dom = domain_word_list\n",
        "    r = rareness[1] / len(list_dom)\n",
        "    l = 0\n",
        "    if domain_len > 10:\n",
        "        l = domain_len - 10\n",
        "    weight_letter = calculateWeightLetters(domain)\n",
        "    y = calculateConsecutive(domain)\n",
        "\n",
        "    weight = w + h + d + r + l + weight_letter - y\n",
        "    return weight\n",
        "\n",
        "# get readable index\n",
        "import re\n",
        "import syllables\n",
        "\n",
        "def getReadableIndex(domain_word_list):\n",
        "    text = ' '.join(domain_word_list)\n",
        "    flesch_reading_ease = 206.835 - 1.015 * (len(re.findall(r'\\b\\w+\\b', text))) - \\\n",
        "                    84.6 * (sum(syllables.estimate(word) for word in re.findall(r'\\b\\w+\\b', text)) / len(re.findall(r'\\b\\w+\\b', text)))\n",
        "    return flesch_reading_ease\n",
        "\n",
        "# get frequency\n",
        "def getDCharD(domain):\n",
        "    dgraph = calculate_digraph_frequency(domain)\n",
        "    trigraph = calculate_digraph_frequency(domain)\n",
        "    chars = calculate_char_frequency(domain)\n",
        "    s_dgraph = sum([value for key, value in dgraph.items()])\n",
        "    s_trigraph = sum([value for key, value in trigraph.items()])\n",
        "    s_chars = sum([value for key, value in chars.items()])\n",
        "    s = s_chars + s_trigraph + s_dgraph\n",
        "    return s/len(domain)\n",
        "\n",
        "# get char frequen in alexa-top 1 mil\n",
        "\n",
        "freq_letter = {\n",
        "    'a': 7.07, 'b': 2.05, 'c': 7.06, 'd': 2.67, 'e': 7.89,\n",
        "    'f': 1.32, 'g': 2.36, 'h': 2.01, 'i': 5.71, 'j': 0.41,\n",
        "    'k': 1.49, 'l': 3.90, 'm': 6.31, 'n': 5.05, 'o': 10.08,\n",
        "    'p': 2.46, 'q': 0.14, 'r': 6.04, 's': 5.14, 't': 5.19,\n",
        "    'u': 2.97, 'v': 1.11, 'w': 0.94, 'x': 0.47, 'y': 1.26,\n",
        "    'z': 0.57, '0': 0.11, '1': 0.16, '2': 0.15, '3': 0.09,\n",
        "    '4': 0.10, '5': 0.06, '6': 0.05, '7': 0.05, '8': 0.06,\n",
        "    '9': 0.05, '-': 0.82, '_': 6.63\n",
        "}\n",
        "\n",
        "def getCharFreq(domain):\n",
        "    freq = 0\n",
        "    for i in domain:\n",
        "        freq+=freq_letter[i]\n",
        "    return freq\n",
        "\n",
        "\n",
        "# calculate entropy\n",
        "import math\n",
        "\n",
        "\n",
        "def getEntropy(domain):\n",
        "    \"Calculates the Shannon entropy of a string\"\n",
        "    # get probability of chars in string\n",
        "    prob = [ float(domain.count(c)) / len(domain) for c in dict.fromkeys(list(domain)) ]\n",
        "    # calculate the entropy\n",
        "    entropy = - sum([ p * math.log(p) / math.log(2.0) for p in prob ])\n",
        "    return entropy\n",
        "\n",
        "\n",
        "\n",
        "def Load():\n",
        "    global words, domain_freq\n",
        "    words = [word.lower() for word in brown.words() if word.isalpha()]\n",
        "    domain_freq = FreqDist(words)\n",
        "\n",
        "    global check_dict\n",
        "    check_dict = enchant.Dict(\"en-US\")\n",
        "    for i in enchant.list_languages():\n",
        "        check_dict.add(i)\n",
        "\n",
        "    wordsegment.load()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnEmf6Xu41jc"
      },
      "outputs": [],
      "source": [
        "def get_domain(domain):\n",
        "    a = domain.split('.')\n",
        "def getFeature(domain, label, domaintype):\n",
        "    domain_word_list = domain2list(domain)\n",
        "    meaning = getMeaningRatio(domain_word_list)\n",
        "    loadRareness(domain_word_list)\n",
        "    features = []\n",
        "    features.append(domain)\n",
        "    features.append(getDomainLength(domain))\n",
        "    features.append(len(domain_word_list))\n",
        "    features.append(getNumOfHyphen(domain))\n",
        "    features.append(getNumOfDigit(domain))\n",
        "    features.append(getRareRatio(domain_word_list))\n",
        "    features.append(getCommonRatio(domain_word_list))\n",
        "    features.append(meaning[0])\n",
        "    features.append(meaning[1])\n",
        "    features.append(getNumberRandomWord())\n",
        "    features.append(getReadableIndex(domain_word_list))\n",
        "    features.append(getWeightDifficult(domain, domain_word_list))\n",
        "    features.append(isDomainTypeIllicit(domaintype))\n",
        "    features.append(getDCharD(domain))\n",
        "    features.append(getCharFreq(domain))\n",
        "    features.append(getEntropy(domain))\n",
        "    features.append(label)\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4QTdIeh-J5w"
      },
      "source": [
        "# 3. Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDGF1GgLHYky"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "main_data = pd.read_csv('/content/drive/MyDrive/dataset/datset1.txt',sep =' ', header= None)\n",
        "main_data.columns = ['domain','type']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lOPNT_-kB9"
      },
      "source": [
        "# 4. Feature collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC6rwbGX15Ta",
        "outputId": "c645786f-2eb9-46b1-c396-c856c7a981b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "\n",
        "\n",
        "feature_names = ['Domain','DomainLength', 'NoOfDomainWords', 'NoOfHyphen',\n",
        "                 'NoOfNumericToken', 'RareRatio', 'CommonRatio',\n",
        "                 'HasMeaningRatio', 'MisspelledRatio', 'NoOfRandom',\n",
        "                 'Readable', 'isDifficult', 'Dtype', 'DCharD',\n",
        "                 'CharFreq', 'Entropy','Label'\n",
        "                 ]\n",
        "\n",
        "domain_features = []\n",
        "Load()\n",
        "for i in range(510122):\n",
        "    a = main_data['domain'][i].split('.')\n",
        "    domain = a[0]\n",
        "    domaintype = a[1]\n",
        "    label = main_data['type'][i]\n",
        "    # i have changed label here. You can keep the label value or whatever you want here.\n",
        "    if(int(label) > 1):\n",
        "        label = 1\n",
        "    print(\"Collecting...\", i, end=\"\\r\")\n",
        "    domain_features.append(getFeature(domain,label,domaintype))\n",
        "\n",
        "domain_data = pd.DataFrame(domain_features, columns=feature_names)\n",
        "\n",
        "domain_data.to_csv('/content/drive/MyDrive/dataset/data0_atm.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUCDHXge3Vvb"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR13Ko87Y40B"
      },
      "outputs": [],
      "source": [
        "data = domain_data.drop(['Domain'],axis=1).copy()\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "y = data['Label']\n",
        "X = data.drop('Label',axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW__No3-3S7b"
      },
      "source": [
        "## Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIbR6eCV3QI8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size = 0.36, random_state = 12)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDsYmXHQGhe1",
        "outputId": "3c820683-07b0-4ec6-eb3e-409485d68b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree: Accuracy on test Data: 0.9640445644834571\n",
            "False Positives: 5134\n",
            "False Negatives: 1469\n",
            "Precision: 0.935\n",
            "Recall: 0.981\n",
            "F1 Score: 0.958\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier(max_depth=5)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "y_test_tree = tree.predict(X_test)\n",
        "\n",
        "acc_test_tree = accuracy_score(y_test,y_test_tree)\n",
        "\n",
        "print(\"Decision Tree: Accuracy on test Data: {}\".format(acc_test_tree))\n",
        "conf_matrix = confusion_matrix(y_test, y_test_tree)\n",
        "\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, y_test_tree)\n",
        "recall = recall_score(y_test, y_test_tree)\n",
        "f1 = f1_score(y_test, y_test_tree)\n",
        "print(\"False Positives: {}\".format(fp))\n",
        "print(\"False Negatives: {}\".format(fn))\n",
        "print(\"Precision: {}\".format(precision))\n",
        "print(\"Recall: {}\".format(recall))\n",
        "print(\"F1 Score: {}\".format(f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## k-NN algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqUc6KhI_ew0",
        "outputId": "ba416f96-5067-4efb-c5b9-1ae305d33124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K-Nearest Neighbors: Accuracy on test data: 0.9654657925115985\n",
            "False Positives: 5134\n",
            "False Negatives: 1469\n",
            "Precision: 0.9354863030912289\n",
            "Recall: 0.9806494105249292\n",
            "F1 Score: 0.9575356120775588\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_test_knn = knn.predict(X_test)\n",
        "\n",
        "acc_test_knn = accuracy_score(y_test, y_test_knn)\n",
        "\n",
        "print(\"K-Nearest Neighbors: Accuracy on test data: {}\".format(acc_test_knn))\n",
        "conf_matrix = confusion_matrix(y_test, y_test_tree)\n",
        "\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, y_test_tree)\n",
        "recall = recall_score(y_test, y_test_tree)\n",
        "f1 = f1_score(y_test, y_test_tree)\n",
        "print(\"False Positives: {}\".format(fp))\n",
        "print(\"False Negatives: {}\".format(fn))\n",
        "print(\"Precision: {}\".format(precision))\n",
        "print(\"Recall: {}\".format(recall))\n",
        "print(\"F1 Score: {}\".format(f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ANN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DlE2v31vd4o",
        "outputId": "f76d3168-854a-4cc6-bee0-4c35ffd42047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial Neural Network: Accuracy on test data: 0.975\n",
            "False Positives: 5134\n",
            "False Negatives: 1469\n",
            "Precision: 0.9354863030912289\n",
            "Recall: 0.9806494105249292\n",
            "F1 Score: 0.9575356120775588\n"
          ]
        }
      ],
      "source": [
        "# Artificial Neural Network\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "ann = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
        "ann.fit(X_train, y_train)\n",
        "\n",
        "y_test_ann = ann.predict(X_test)\n",
        "\n",
        "acc_test_ann = accuracy_score(y_test, y_test_ann)\n",
        "\n",
        "print(\"Artificial Neural Network: Accuracy on test data: {:.3f}\".format(acc_test_ann))\n",
        "conf_matrix = confusion_matrix(y_test, y_test_tree)\n",
        "\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, y_test_tree)\n",
        "recall = recall_score(y_test, y_test_tree)\n",
        "f1 = f1_score(y_test, y_test_tree)\n",
        "print(\"False Positives: {}\".format(fp))\n",
        "print(\"False Negatives: {}\".format(fn))\n",
        "print(\"Precision: {}\".format(precision))\n",
        "print(\"Recall: {}\".format(recall))\n",
        "print(\"F1 Score: {}\".format(f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4twWyRLCyxHt",
        "outputId": "4031bbe0-904c-44c3-e6ab-9ef3502c2135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Support Vector Machine: Accuracy on test data: 0.865\n",
            "False Positives: 5134\n",
            "False Negatives: 1469\n",
            "Precision: 0.9354863030912289\n",
            "Recall: 0.9806494105249292\n",
            "F1 Score: 0.9575356120775588\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "svm = SVC(kernel='poly')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_test_svm = svm.predict(X_test)\n",
        "\n",
        "acc_test_svm = accuracy_score(y_test, y_test_svm)\n",
        "\n",
        "print(\"Support Vector Machine: Accuracy on test data: {:.3f}\".format(acc_test_svm))\n",
        "conf_matrix = confusion_matrix(y_test, y_test_tree)\n",
        "\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, y_test_tree)\n",
        "recall = recall_score(y_test, y_test_tree)\n",
        "f1 = f1_score(y_test, y_test_tree)\n",
        "print(\"False Positives: {}\".format(fp))\n",
        "print(\"False Negatives: {}\".format(fn))\n",
        "print(\"Precision: {}\".format(precision))\n",
        "print(\"Recall: {}\".format(recall))\n",
        "print(\"F1 Score: {}\".format(f1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "JJP0esLqTFQ9",
        "outputId": "46780b33-854c-4688-b473-ec1bec69a056"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-184e8081f1d0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_test_svm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'acc_test_svm' is not defined"
          ]
        }
      ],
      "source": [
        "print(acc_test_svm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
